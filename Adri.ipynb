{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0d7e7-15bd-4dc1-8a85-1a09d8c4bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ec5c6-377b-4c93-a1ef-32dc6cb9f552",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c1619-c8b1-4931-b53a-c4debe1203c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_layers: list = [128, 64],\n",
    "                 output_size: int = 2,\n",
    "                 activation: str = \"relu\",\n",
    "                 dropout: float = 0.0,\n",
    "                 normalization: str = None,   # \"batch\", \"layer\" o None\n",
    "                 skip_connections: bool = False):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # Selección de función de activación\n",
    "        activations = {\n",
    "            \"relu\": nn.ReLU,\n",
    "            \"tanh\": nn.Tanh,\n",
    "            \"leakyrelu\": nn.LeakyReLU,\n",
    "            \"gelu\": nn.GELU\n",
    "        }\n",
    "        act_fn = activations.get(activation.lower(), nn.ReLU)\n",
    "\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        self.skip_connections = skip_connections\n",
    "        self.hidden_dims = hidden_layers\n",
    "\n",
    "        # Construcción dinámica de capas ocultas\n",
    "        for hidden_dim in hidden_layers:\n",
    "            block = []\n",
    "            block.append(nn.Linear(in_features, hidden_dim))\n",
    "\n",
    "            # Normalización opcional\n",
    "            if normalization == \"batch\":\n",
    "                block.append(nn.BatchNorm1d(hidden_dim))\n",
    "            elif normalization == \"layer\":\n",
    "                block.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "            block.append(act_fn())\n",
    "\n",
    "            if dropout > 0:\n",
    "                block.append(nn.Dropout(dropout))\n",
    "\n",
    "            layers.append(nn.Sequential(*block))\n",
    "            in_features = hidden_dim\n",
    "\n",
    "        self.hidden_blocks = nn.ModuleList(layers)\n",
    "        self.output_layer = nn.Linear(in_features, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        prev_out = None\n",
    "\n",
    "        for i, block in enumerate(self.hidden_blocks):\n",
    "            new_out = block(out)\n",
    "            if self.skip_connections and prev_out is not None and new_out.shape == prev_out.shape:\n",
    "                out = new_out + prev_out  # skip connection\n",
    "            else:\n",
    "                out = new_out\n",
    "            prev_out = out\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54ca09-7f5a-4d5a-a7b8-817aaa26ec1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21901bbb-c333-4579-8f5b-661075b0714f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce6110-4c0a-45f8-bb87-e4f3eff39bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa781fff-7c1c-48a9-81ce-ac0c084cc09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff7139-2350-404b-8b32-3eb1299c9432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46421271-9138-48b7-ab83-be7765a8c86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469dbe7-ab97-448f-b95d-9ac0026de3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e5fe7-3f89-4ee8-9613-7dfd152f999d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb973ce3-64f8-42a0-b02d-df662f83d37d",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3c807-3bcc-4f33-a01a-15a6cde8b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear layer (Dense, Fully connected, Single Layer Perceptron)\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.dW = np.zeros_like(self.W) # Watch-out for the shape - it has to be same as W\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "        self.db = np.zeros_like(self.b) # Watch-out for the shape - it has to be same as b\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_inputs = input\n",
    "        self.m = self.fw_inputs.shape[1]\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        self.dW = (1.0/self.m) * np.matmul(dz, self.fw_inputs.T)\n",
    "        self.db = (1.0/self.m) * np.sum(dz, axis=1, keepdims=True)\n",
    "        return np.matmul(self.W.T, dz)\n",
    "\n",
    "    def get_optimizer_context(self):\n",
    "        return [[self.W, self.dW], [self.b, self.db]]\n",
    "\n",
    "    def set_optimizer_context(self, params):\n",
    "        self.W, self.b = params\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, dz: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            dz = module.backward(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e751aaa-81d2-49a9-a1cb-0f4d46dbb9d6",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210c158-48a2-463e-b3c3-f58bb47fc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, np.multiply(a, 1 - a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   HyperbolicTangentActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2 * input) - 1) / (np.exp(2 * input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, 1 - np.square(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28468b28-d140-400e-bd40-e065899a5faf",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ee24c-8e90-40f8-adee-9afd82d4cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   MeanSquareErrorLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class MSELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(MSELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(np.power(target - input, 2))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -2 * (target - input)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BCELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(BCELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(-(target * np.log(input) + np.multiply((1 - target), np.log(1 - input))))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -np.divide(target, input) + np.divide(1 - target, 1 - input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081819e5-23de-4857-91b3-7e98579c9a9a",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70706c8-ef2d-45ba-abb3-5ea057d68b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SGDMomentumOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, lr, beta):\n",
    "        super(SGDMomentum, self).__init__()\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self, model):\n",
    "        for name, layer in model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "\n",
    "                    # >>>> start_solution\n",
    "\n",
    "                    # Initialize\n",
    "                    if (not name in self.context.keys()):\n",
    "                        self.context[name] = {\n",
    "                            # ??? Something reasonable should be here !\n",
    "                            'V': [0, 0]\n",
    "                        }\n",
    "\n",
    "                    VdW, Vdb = self.context[name]['V']\n",
    "\n",
    "                    VdW = (1 - self.beta)*dW + self.beta*VdW\n",
    "                    Vdb = (1 - self.beta)*db + self.beta*Vdb\n",
    "\n",
    "                    # Update our exponential averages\n",
    "                    self.context[name]['V'] =  [VdW, Vdb] # Some kind of computation\n",
    "\n",
    "                    # Update parameters\n",
    "                    W = W - self.lr * VdW # Each optimizer is a little bit different !\n",
    "                    b = b - self.lr * Vdb # Each optimizer is a little bit different !\n",
    "                        \n",
    "                    # <<<< end_solution\n",
    "                    layer.set_optimizer_context([W,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82943cc-ca40-4cd1-b0f5-19de02563d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   RMSpropOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, lr, beta):\n",
    "        super(RMSprop, self).__init__()\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self, model):\n",
    "        for name, layer in model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "\n",
    "                    # >>>> start_solution\n",
    "\n",
    "                    # Initialize\n",
    "                    if (not name in self.context.keys()):\n",
    "                        self.context[name] = {\n",
    "                            'S': [0, 0]# ??? Something reasonable should be here !\n",
    "                        }\n",
    "                        \n",
    "                    SdW, Sdb = self.context[name]['S']\n",
    "\n",
    "                    SdW = (1 - self.beta)*(dW**2) + self.beta*SdW\n",
    "                    Sdb = (1 - self.beta)*(db**2) + self.beta*Sdb\n",
    "\n",
    "                    # Update our exponential averages\n",
    "                    self.context[name]['S'] =  [SdW, Sdb] # Some kind of computation\n",
    "\n",
    "                    # Update parameters\n",
    "                    W = W - self.lr * (dW/(SdW**0.5)) # Each optimizer is a little bit different !\n",
    "                    b = b - self.lr * (db/(Sdb**0.5)) # Each optimizer is a little bit different !\n",
    "\n",
    "                    # <<<< end_solution\n",
    "                    layer.set_optimizer_context([W, b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ed238-af4e-49af-bab6-47ce2227248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   AdamOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, lr, betaV, betaS):\n",
    "        super(Adam, self).__init__()\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "        self.lr = lr\n",
    "        self.betaV = betaV\n",
    "        self.betaS = betaS\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self, model):\n",
    "        # >>>>>> Probably add something here ;)\n",
    "\n",
    "        # <<<<<< until here\n",
    "        for name, layer in model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "\n",
    "                    # >>>> start_solution\n",
    "\n",
    "                    # Initialize\n",
    "                    if (not name in self.context.keys()):\n",
    "                        self.context[name] = {\n",
    "                            'V': [0, 0],\n",
    "                            'S': [0, 0]  \n",
    "                        }\n",
    "\n",
    "                    VdW, Vdb = self.context[name]['V']\n",
    "                    SdW, Sdb = self.context[name]['S']\n",
    "\n",
    "                    VdW = (1 - self.betaV)*dW + self.betaV*VdW\n",
    "                    Vdb = (1 - self.betaV)*db + self.betaV*Vdb\n",
    "                    SdW = (1 - self.betaS)*(dW**2) + self.betaS*SdW\n",
    "                    Sdb = (1 - self.betaS)*(db**2) + self.betaS*Sdb\n",
    "                    \n",
    "\n",
    "                    # Update our exponential averages\n",
    "                    self.context[name]['V'] =  [VdW, Vdb] # Some kind of computation\n",
    "                    self.context[name]['S'] =  [SdW, Sdb]\n",
    "\n",
    "                    self.t += 1\n",
    "\n",
    "                    # Adam does something with the averages\n",
    "                    VdW_ = VdW/(1 - self.betaV**self.t)\n",
    "                    SdW_ = SdW/(1 - self.betaS**self.t)\n",
    "                    Vdb_ = Vdb/(1 - self.betaV**self.t)\n",
    "                    Sdb_ = Sdb/(1 - self.betaS**self.t)\n",
    "\n",
    "                    # Update parameters\n",
    "                    W = W - self.lr * (VdW_/(SdW_**0.5 + self.epsilon)) # Each optimizer is a little bit different !\n",
    "                    b = b - self.lr * (Vdb_/(Sdb_**0.5 + self.epsilon)) # Each optimizer is a little bit different !\n",
    "\n",
    "                    # <<<< end_solution\n",
    "                    layer.set_optimizer_context([W, b])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
