{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2887197b-03dc-45df-a532-40083d5a1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ec5c6-377b-4c93-a1ef-32dc6cb9f552",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c98edf-04be-46a4-9c8f-f662c68e22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # --- ARQUITECTURA ---\n",
    "        self.input_dim = 100\n",
    "        self.output_dim = 10\n",
    "        self.hidden_layers = [128, 64]  # número y tamaño de capas\n",
    "        self.activation = 'relu'        # relu, tanh, sigmoid, leakyrelu\n",
    "        self.weight_init = 'xavier'     # he, xavier, normal, uniform\n",
    "        self.use_batch_norm = True\n",
    "        self.dropout_rate = 0.3\n",
    "        self.skip_connections = False\n",
    "\n",
    "        # --- OPTIMIZACIÓN ---\n",
    "        self.optimizer = 'adam'         # sgd, adam, rmsprop, adamw\n",
    "        self.learning_rate = 1e-3\n",
    "        self.momentum = 0.9\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 50\n",
    "        self.gradient_clipping = 1.0\n",
    "        self.scheduler = 'steplr'       # steplr, cosine, none\n",
    "\n",
    "        # --- REGULARIZACIÓN ---\n",
    "        self.L1_lambda = 0.0\n",
    "        self.L2_lambda = 1e-4\n",
    "        self.early_stopping_patience = 5\n",
    "\n",
    "        # --- FUNCIÓN DE PÉRDIDA ---\n",
    "        self.loss_function = 'crossentropy'  # mse, crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e3c807-3bcc-4f33-a01a-15a6cde8b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = config.input_dim\n",
    "        self.config = config\n",
    "        self.skip_connections = config.skip_connections\n",
    "\n",
    "        for i, hidden_dim in enumerate(config.hidden_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "\n",
    "            if config.use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "            act = self._get_activation(config.activation)\n",
    "            layers.append(act)\n",
    "\n",
    "            if config.dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(config.dropout_rate))\n",
    "\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(in_dim, config.output_dim)\n",
    "\n",
    "        self._initialize_weights(config.weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.skip_connections:\n",
    "            x = self.hidden(x)\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            # Skip connections simples (residual sum)\n",
    "            out = x\n",
    "            for layer in self.hidden:\n",
    "                prev = out\n",
    "                out = layer(out)\n",
    "                if isinstance(layer, nn.Linear) and prev.shape == out.shape:\n",
    "                    out = out + prev\n",
    "            return self.output_layer(out)\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        name = name.lower()\n",
    "        if name == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif name == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif name == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif name == 'leakyrelu':\n",
    "            return nn.LeakyReLU(0.01)\n",
    "        else:\n",
    "            raise ValueError(f\"Función de activación no soportada: {name}\")\n",
    "\n",
    "    def _initialize_weights(self, init_type):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if init_type == 'xavier':\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                elif init_type == 'he':\n",
    "                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                elif init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                elif init_type == 'uniform':\n",
    "                    nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
