{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2887197b-03dc-45df-a532-40083d5a1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ec5c6-377b-4c93-a1ef-32dc6cb9f552",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c98edf-04be-46a4-9c8f-f662c68e22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # --- ARQUITECTURA ---\n",
    "        self.input_dim = 100\n",
    "        self.output_dim = 10\n",
    "        self.hidden_layers = [128, 64]  # número y tamaño de capas\n",
    "        self.activation = 'relu'        # relu, tanh, sigmoid, leakyrelu\n",
    "        self.weight_init = 'xavier'     # he, xavier, normal, uniform\n",
    "        self.use_batch_norm = True\n",
    "        self.dropout_rate = 0.3\n",
    "        self.skip_connections = False\n",
    "\n",
    "        # --- OPTIMIZACIÓN ---\n",
    "        self.optimizer = 'adam'         # sgd, adam, rmsprop, adamw\n",
    "        self.learning_rate = 1e-3\n",
    "        self.momentum = 0.9\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 50\n",
    "        self.gradient_clipping = 1.0\n",
    "        self.scheduler = 'steplr'       # steplr, cosine, none\n",
    "\n",
    "        # --- REGULARIZACIÓN ---\n",
    "        self.L1_lambda = 0.0\n",
    "        self.L2_lambda = 1e-4\n",
    "        self.early_stopping_patience = 5\n",
    "\n",
    "        # --- FUNCIÓN DE PÉRDIDA ---\n",
    "        self.loss_function = 'crossentropy'  # mse, crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e3c807-3bcc-4f33-a01a-15a6cde8b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = config.input_dim\n",
    "        self.config = config\n",
    "        self.skip_connections = config.skip_connections\n",
    "\n",
    "        for i, hidden_dim in enumerate(config.hidden_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "\n",
    "            if config.use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "            act = self._get_activation(config.activation)\n",
    "            layers.append(act)\n",
    "\n",
    "            if config.dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(config.dropout_rate))\n",
    "\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(in_dim, config.output_dim)\n",
    "\n",
    "        self._initialize_weights(config.weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.skip_connections:\n",
    "            x = self.hidden(x)\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            # Skip connections simples (residual sum)\n",
    "            out = x\n",
    "            for layer in self.hidden:\n",
    "                prev = out\n",
    "                out = layer(out)\n",
    "                if isinstance(layer, nn.Linear) and prev.shape == out.shape:\n",
    "                    out = out + prev\n",
    "            return self.output_layer(out)\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        name = name.lower()\n",
    "        if name == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif name == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif name == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif name == 'leakyrelu':\n",
    "            return nn.LeakyReLU(0.01)\n",
    "        else:\n",
    "            raise ValueError(f\"Función de activación no soportada: {name}\")\n",
    "\n",
    "    def _initialize_weights(self, init_type):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if init_type == 'xavier':\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                elif init_type == 'he':\n",
    "                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                elif init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                elif init_type == 'uniform':\n",
    "                    nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bc239f-4b40-4da5-af47-c9d8170c21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, train_data, val_data=None, device='cpu'):\n",
    "    model.to(device)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    if config.loss_function == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate,\n",
    "                               betas=(config.beta1, config.beta2), eps=config.epsilon,\n",
    "                               weight_decay=config.L2_lambda)\n",
    "    elif config.optimizer == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    elif config.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config.learning_rate,\n",
    "                              momentum=config.momentum, weight_decay=config.L2_lambda)\n",
    "    elif config.optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizador no soportado: {config.optimizer}\")\n",
    "\n",
    "    # Scheduler\n",
    "    if config.scheduler == 'steplr':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    elif config.scheduler == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_data, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=config.batch_size) if val_data else None\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "\n",
    "            # Regularización L1\n",
    "            if config.L1_lambda > 0:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss = loss + config.L1_lambda * l1_norm\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            if config.gradient_clipping:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clipping)\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    outputs = model(xb)\n",
    "                    loss = criterion(outputs, yb)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            print(f\"Epoch [{epoch+1}/{config.epochs}] - \"\n",
    "                  f\"Train Loss: {train_loss/len(train_loader):.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= config.early_stopping_patience:\n",
    "                    print(\"⏹️ Early stopping activado.\")\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Epoch [{epoch+1}/{config.epochs}] - Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
